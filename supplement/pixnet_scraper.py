import requests
import time
import csv
import os
from collections import defaultdict, Counter
from bs4 import BeautifulSoup
from tqdm import tqdm

QUERIES = [
    "è‡ªå¸¶é…’æ°´", "è‡ªå¸¶é…’", "å¸¶é…’", "BYOB", "è‡ªå‚™é…’", "å…é–‹ç“¶è²»",
    "å¯å¸¶é…’", "é…’è‡ªå¸¶", "æ”œå¸¶é…’", "ä¾é…’è²»", "æ´—æ¯è²»",
    "é…’æ¯æœå‹™", "æä¾›é…’æ¯", "æ”¶æ¯è²»", "å…é…’æ¯è²»"
]

PIXNET_HITS_PATH = "data/pixnet_hits.csv"
PIXNET_SUMMARY_PATH = "data/pixnet_summary.csv"
LOG_PATH = "data/pixnet_log.txt"
SEED_LIST_PATH = "data/seed_list_raw.txt"

FAST_MODE = True  # True = åªç”¨ meta ç¯©é¸ï¼›False = ä¸€å¾‹æ·±å…¥å…§æ–‡åˆ†æ


def read_seed_list(path):
    with open(path, 'r', encoding='utf-8') as f:
        return [line.split('#')[0].strip() for line in f if line.strip()]


def search_pixnet(broad_query, target_name, num_results=10, in_content=False):
    api_key = "7583e8557b72d1542cb957969f3a70df8ad0156866dc5cdfbf3103a1e5074ca4"
    params = {
        "engine": "google",
        "q": f"{broad_query} site:pixnet.net",
        "api_key": api_key,
        "hl": "zh-tw",
        "num": num_results
    }
    response = requests.get("https://serpapi.com/search", params=params)
    data = response.json()

    results = []
    for item in data.get("organic_results", []):
        title = item.get("title", "")
        link = item.get("link", "")
        snippet = item.get("snippet", "")

        contains_in_meta = target_name in title or target_name in link or target_name in snippet
        results.append({
            "restaurant": target_name,
            "keyword": broad_query,
            "title": title,
            "url": link,
            "snippet": snippet,
            "meta_match": contains_in_meta
        })
    return results


def fetch_pixnet_article(url):
    headers = {"User-Agent": "Mozilla/5.0"}
    response = requests.get(url, headers=headers)
    soup = BeautifulSoup(response.text, 'html.parser')

    article_body = soup.find('div', class_='article-content-inner') or \
                   soup.find('div', class_='article-content') or \
                   soup.find('div', class_='content')

    if not article_body:
        return ""

    paragraphs = article_body.find_all('p')
    content = "\n".join(p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True))
    return content.strip()


def deduplicate_hits(hits):
    combined_hits = {}
    for hit in hits:
        key = (hit["restaurant"], hit["url"])
        if key not in combined_hits:
            combined_hits[key] = {
                "restaurant": hit["restaurant"],
                "keywords": set([hit["keyword"]]),
                "title": hit["title"],
                "url": hit["url"],
                "snippet": hit["snippet"]
            }
        else:
            combined_hits[key]["keywords"].add(hit["keyword"])
    return list(combined_hits.values())


def save_hits_to_csv(hits, path, overwrite=False):
    mode = 'w' if overwrite else 'a'
    write_header = overwrite or not os.path.exists(path)
    with open(path, mode=mode, newline='', encoding='utf-8') as f:
        writer = csv.DictWriter(f, fieldnames=["é¤å»³åç¨±", "å‘½ä¸­é—œéµå­—", "æ–‡ç« æ¨™é¡Œ", "ç¶²å€", "å…§æ–‡æ‘˜è¦"])
        if write_header:
            writer.writeheader()
        for hit in hits:
            writer.writerow({
                "é¤å»³åç¨±": hit["restaurant"],
                "å‘½ä¸­é—œéµå­—": ";".join(sorted(hit["keywords"])),
                "æ–‡ç« æ¨™é¡Œ": hit["title"],
                "ç¶²å€": hit["url"],
                "å…§æ–‡æ‘˜è¦": hit["snippet"]
            })


def log_message(message):
    print(message)
    with open(LOG_PATH, 'a', encoding='utf-8') as log_file:
        log_file.write(message + '\n')


if __name__ == "__main__":
    targets = read_seed_list(SEED_LIST_PATH)

    all_hits_total = []
    keyword_counter = Counter()
    with open(LOG_PATH, 'w', encoding='utf-8') as f:
        f.write("Pixnet å‘½ä¸­ç´€éŒ„\n\n")

    for target_name in tqdm(targets, desc="æŸ¥è©¢é€²åº¦"):
        log_message(f"\nğŸ” é¤å»³ï¼š{target_name}")
        phase1_hits = []
        final_hits = []

        for broad_query in QUERIES:
            log_message(f"â†’ å˜—è©¦é—œéµå­—ï¼š{broad_query}")
            meta_hits = search_pixnet(broad_query, target_name, in_content=False)
            filtered = [hit for hit in meta_hits if hit["meta_match"] and "pixnet.net" in hit["url"]]
            phase1_hits.extend(filtered)
            time.sleep(1)

        for hit in phase1_hits:
            content = fetch_pixnet_article(hit["url"]) if not FAST_MODE else ""
            if FAST_MODE or target_name in content:
                hit["snippet"] = content[:300] if content else hit["snippet"]
                final_hits.append(hit)
                keyword_counter[hit["keyword"]] += 1
                log_message(f"âœ… å‘½ä¸­ï¼š{hit['title']} â†’ {hit['url']}")

        if final_hits:
            deduped_hits = deduplicate_hits(final_hits)
            all_hits_total.extend(deduped_hits)
        else:
            log_message("âŒ ç„¡å‘½ä¸­")

    if all_hits_total:
        save_hits_to_csv(all_hits_total, PIXNET_HITS_PATH, overwrite=True)
        print("\nâœ… å®Œæˆè¼¸å‡ºï¼Œå…± {} ç­†å‘½ä¸­çµæœã€‚".format(len(all_hits_total)))
        print("ğŸ“Š å‘½ä¸­é—œéµå­—çµ±è¨ˆï¼š")
        for kw, count in keyword_counter.items():
            print(f"{kw}ï¼š{count} ç­†")
    else:
        print("âŒ ç„¡å‘½ä¸­çµæœã€‚")
